{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMiEmdLDem1AL8XmbPta9tl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rogerjordan123/html/blob/master/Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9V4U_1WEUsM",
        "outputId": "72427e5a-105b-465c-c8bf-1e76bf951cdb"
      },
      "source": [
        "import numpy as np\n",
        "!pip install python-mnist\n",
        "from mnist import MNIST"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-mnist\n",
            "  Downloading https://files.pythonhosted.org/packages/64/f0/6086b84427c3bf156ec0b3c2f9dfc1d770b35f942b9ed8a64f5229776a80/python_mnist-0.7-py2.py3-none-any.whl\n",
            "Installing collected packages: python-mnist\n",
            "Successfully installed python-mnist-0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40Bcp2B548BI"
      },
      "source": [
        "mndata = MNIST('/content')\n",
        "\n",
        "X_train, y_train = mndata.load_training()\n",
        "X_test, y_test = mndata.load_testing()\n",
        "\n",
        "X_train, y_train, X_test, y_test = np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test)\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsGSypAwLRmD",
        "outputId": "df874f4f-ba8a-4eb6-c55c-84fb6e1dbf11"
      },
      "source": [
        "# MNIST data loader\n",
        "#\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import struct\n",
        "from array import array\n",
        "from os.path  import join\n",
        "\n",
        "def to_categorical(a, num_classes):\n",
        "    return np.squeeze(np.eye(num_classes)[a.reshape(-1)])   \n",
        "\n",
        "#\n",
        "# MNIST Data Loader Class\n",
        "#\n",
        "class MnistDataloader(object):\n",
        "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
        "                 test_images_filepath, test_labels_filepath):\n",
        "        self.training_images_filepath = training_images_filepath\n",
        "        self.training_labels_filepath = training_labels_filepath\n",
        "        self.test_images_filepath = test_images_filepath\n",
        "        self.test_labels_filepath = test_labels_filepath\n",
        "    \n",
        "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
        "        labels = []\n",
        "        with open(labels_filepath, 'rb') as file:\n",
        "            magic, size = struct.unpack(\">II\", file.read(8))\n",
        "            if magic != 2049:\n",
        "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
        "            labels = array(\"B\", file.read())        \n",
        "        \n",
        "        with open(images_filepath, 'rb') as file:\n",
        "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
        "            if magic != 2051:\n",
        "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
        "            image_data = array(\"B\", file.read())        \n",
        "        images = []\n",
        "        for i in range(size):\n",
        "            images.append([0] * rows * cols)\n",
        "        for i in range(size):\n",
        "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
        "            img = img.reshape(28, 28)\n",
        "            images[i][:] = img            \n",
        "        \n",
        "        return images, labels\n",
        "            \n",
        "    def load_data(self):\n",
        "        print('Reading training data...')\n",
        "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
        "        print('Reading test data...')\n",
        "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
        "        print('MNIST data loaded.')\n",
        "        return (x_train, y_train),(x_test, y_test)  \n",
        "\n",
        "# Set file paths based on added MNIST Datasets\n",
        "input_path = '../input'\n",
        "training_images_filepath = join(input_path, '/content/train-images.idx3-ubyte')\n",
        "training_labels_filepath = join(input_path, '/content/train-labels.idx1-ubyte')\n",
        "test_images_filepath = join(input_path, '/content/train-images.idx3-ubyte')\n",
        "test_labels_filepath = join(input_path, '/content/t10k-labels.idx1-ubyte')\n",
        "\n",
        "# Load MINST dataset\n",
        "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
        "(X_train, y_train), (X_test, y_test) = mnist_dataloader.load_data()\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = np.array([x.flatten() for x in X_train])\n",
        "y_train = np.array([to_categorical(y, 10) for y in y_train], dtype=int)\n",
        "\n",
        "X_test = np.array([x.flatten() for x in X_test])\n",
        "y_test = np.array([to_categorical(y, 10) for y in y_test], dtype=int)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading training data...\n",
            "Reading test data...\n",
            "MNIST data loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mS-d4Sp76Kg",
        "outputId": "3297d349-0060-4497-fc48-a461942ccfb3"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd3LB4DbL1dF"
      },
      "source": [
        "X_train = X_train[0:5000,:]\n",
        "X_test = X_test[0:500,:]\n",
        "y_train = y_train[0:5000]\n",
        "y_test = y_test[0:500]"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFoANlLgMZ-Z"
      },
      "source": [
        "class NeuralNetwork(object):\n",
        "\n",
        "  def __init__(self,X,y):\n",
        "    # m para os treinos de exemplos (quantas obsevações temos)\n",
        "    self.m = X.shape[0]\n",
        "\n",
        "    # n para numero de features no input data (32x32=784)\n",
        "    self.n = X.shape[1]\n",
        "\n",
        "    # h1 para o tamanho da sua primeira camada escondida\n",
        "    self.h1 = 50\n",
        "\n",
        "    # h2 para o tamanho da segunda camada escondida\n",
        "    #self.h2 = 25\n",
        "\n",
        "    # h3 para o no de saida (10 because pois nos temos 10 digitos diferentes)\n",
        "    self.h2 = 10\n",
        "\n",
        "    # learning_rate\n",
        "    self.learning_rate = 1e-5\n",
        "\n",
        "  def initialize_weights(self,l0,l1):\n",
        "    w = np.random.randn(l0,l1)*0.01\n",
        "    b = np.zeros((1,l1))\n",
        "\n",
        "    return w,b\n",
        "    \n",
        "\n",
        "  def forward_prop(self,X,parameters):\n",
        "    W2 = parameters['W2']\n",
        "    W1 = parameters['W1']\n",
        "    b2 = parameters['b2']\n",
        "    b1 = parameters['b1']\n",
        "\n",
        "    #forward prob\n",
        "    a0 = X\n",
        "    z1 = np.dot(a0,W1) + b1\n",
        "\n",
        "    # aplicando a não linearidade relu\n",
        "    a1 = np.maximum(0,z1)\n",
        "    z2 = np.dot(a1,W2) + b2\n",
        "\n",
        "    #softmax na the ultima camada\n",
        "    scores = z2\n",
        "    exp_scores = np.exp(scores)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "    #valores caches fromward pass to use for backward pass\n",
        "    cache = {'a0' : X,\n",
        "             'probs' : probs,\n",
        "             'a1' : a1\n",
        "    }\n",
        "\n",
        "    return cache, probs\n",
        "\n",
        "  def compute_cost(self,y,probs,parameters):\n",
        "    W2 = parameters['W2']\n",
        "    W1 = parameters['W1']\n",
        "\n",
        "    loss = -np.log(probs[np.arange(self.m),y])\n",
        "    avg_loss = np.sum(loss) /self.m\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "  def backward_prop(self, cache, parameters, y):\n",
        "\n",
        "    #descarregamento a partir dos parametros\n",
        "    W2 = parameters['W2']\n",
        "    W1 = parameters['W1']\n",
        "    b2 = parameters['b2']\n",
        "    b1 = parameters['b1']\n",
        "\n",
        "    # startar as probabilidades a partir das probabilidades forward\n",
        "    a0 = cache['a0']\n",
        "    a1 = cache['a1']\n",
        "    probs = cache['probs']\n",
        "\n",
        "    # startar o backpropagation\n",
        "    dz2 = probs\n",
        "    dz2[np.arange(self.m),y] -= 1\n",
        "    dz2 /= self.m\n",
        "\n",
        "    # backprop a partir dos valores dW2 e db2\n",
        "    dW2 = np.dot(a1.T, dz2)\n",
        "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
        "\n",
        "    # backprop na ultima camada \n",
        "    dz1 = np.dot(dz2, W2.T)\n",
        "    dz1 = dz1*(a1 > 0)\n",
        "\n",
        "    # Backpropagation atraves do dW1, db1\n",
        "    dW1 = np.dot(a0.T, dz1)\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "    grads = {\n",
        "        'dW1' : dW1,\n",
        "        'dW2' : dW2,\n",
        "        'db1' : db1,\n",
        "        'db2' : db2\n",
        "    }\n",
        "\n",
        "    return grads\n",
        "\n",
        "  def update_parameters(self,parameters,grads):\n",
        "    learning_rate=self.learning_rate\n",
        "\n",
        "    W2 = parameters['W2']\n",
        "    W1 = parameters['W1']\n",
        "    b2 = parameters['b2']\n",
        "    b1 = parameters['b1']\n",
        "\n",
        "    dW2 = grads['dW2']\n",
        "    dW1 = grads['dW1']\n",
        "    db2 = grads['db2']\n",
        "    db1 = grads['db1']\n",
        "\n",
        "\n",
        "    # gradient descent\n",
        "    W2 -= learning_rate*dW2\n",
        "    W1 -= learning_rate*dW1\n",
        "\n",
        "    b2 -= learning_rate*db2\n",
        "    b1 -= learning_rate*db1\n",
        "\n",
        "    parameters = {\n",
        "        'W1': W1, 'W2': W2, 'b1':b1 , 'b2':b2\n",
        "    }\n",
        "\n",
        "    return parameters\n",
        "  \n",
        "\n",
        "\n",
        "  def main(self,X,y,num_iter):\n",
        "    # inicializar nossos pesos\n",
        "\n",
        "    W1, b1 = self.initialize_weights(self.n, self.h1)\n",
        "    W2, b2 = self.initialize_weights(self.h1, self.h2)\n",
        "    \n",
        "    # pack parameters into a dictionary\n",
        "    parameters = {'W1':W1, 'W2':W2, 'b1':b1,'b2':b2}\n",
        "\n",
        "    # How many gradients descent updates we want to do\n",
        "    for it in range(num_iter+1):\n",
        "\n",
        "      # forward prop\n",
        "      cache, probs = self.forward_prop(X, parameters)\n",
        "\n",
        "      # calculate cost\n",
        "      J = self.compute_cost(y, probs, parameters)\n",
        "\n",
        "      #print cost sometimes\n",
        "      if it % 100 == 0:\n",
        "        print(f'Na interação {it} we have a loss of {J}')\n",
        "\n",
        "      #back propr\n",
        "      grads = self.backward_prop(cache, parameters, y)\n",
        "\n",
        "      #update oaraneterss\n",
        "      parameters = self.update_parameters(parameters, grads)\n",
        "\n",
        "    \n",
        "    return parameters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxVJVYQy8EuX",
        "outputId": "9932402d-530a-488f-8e9f-bef47f38ca9b"
      },
      "source": [
        "NN = NeuralNetwork(X_train, y_train)\n",
        "\n",
        "trained_parameters = NN.main(X_train, y_train, 5000)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Na interação 0 we have a loss of 3.367016318876067\n",
            "Na interação 100 we have a loss of 2.4946851030295885\n",
            "Na interação 200 we have a loss of 2.193626450411557\n",
            "Na interação 300 we have a loss of 1.9840536358780387\n",
            "Na interação 400 we have a loss of 1.808634504285898\n",
            "Na interação 500 we have a loss of 1.6555361762581724\n",
            "Na interação 600 we have a loss of 1.5205434554770407\n",
            "Na interação 700 we have a loss of 1.402483461281722\n",
            "Na interação 800 we have a loss of 1.2997720051064892\n",
            "Na interação 900 we have a loss of 1.210445758806331\n",
            "Na interação 1000 we have a loss of 1.1330397776576784\n",
            "Na interação 1100 we have a loss of 1.065835203551641\n",
            "Na interação 1200 we have a loss of 1.0072951842963243\n",
            "Na interação 1300 we have a loss of 0.9558198308163344\n",
            "Na interação 1400 we have a loss of 0.9104866958516388\n",
            "Na interação 1500 we have a loss of 0.8703855036420708\n",
            "Na interação 1600 we have a loss of 0.83446440858331\n",
            "Na interação 1700 we have a loss of 0.8022246718743403\n",
            "Na interação 1800 we have a loss of 0.7730640930499251\n",
            "Na interação 1900 we have a loss of 0.7465580001669433\n",
            "Na interação 2000 we have a loss of 0.7223294148035247\n",
            "Na interação 2100 we have a loss of 0.7000732718469964\n",
            "Na interação 2200 we have a loss of 0.6795776240429867\n",
            "Na interação 2300 we have a loss of 0.6606384574875823\n",
            "Na interação 2400 we have a loss of 0.6430254059719523\n",
            "Na interação 2500 we have a loss of 0.6265709684949845\n",
            "Na interação 2600 we have a loss of 0.6111520263937739\n",
            "Na interação 2700 we have a loss of 0.5966738357634528\n",
            "Na interação 2800 we have a loss of 0.5830533277260993\n",
            "Na interação 2900 we have a loss of 0.570212891335866\n",
            "Na interação 3000 we have a loss of 0.5580531622388971\n",
            "Na interação 3100 we have a loss of 0.5465229644547002\n",
            "Na interação 3200 we have a loss of 0.5355612812572333\n",
            "Na interação 3300 we have a loss of 0.5251432229922227\n",
            "Na interação 3400 we have a loss of 0.5151832718413839\n",
            "Na interação 3500 we have a loss of 0.5056818149927853\n",
            "Na interação 3600 we have a loss of 0.4966205284319428\n",
            "Na interação 3700 we have a loss of 0.4879552810625762\n",
            "Na interação 3800 we have a loss of 0.4796571144939659\n",
            "Na interação 3900 we have a loss of 0.4717098121445989\n",
            "Na interação 4000 we have a loss of 0.46407781991696256\n",
            "Na interação 4100 we have a loss of 0.45672664630108045\n",
            "Na interação 4200 we have a loss of 0.4496484465513346\n",
            "Na interação 4300 we have a loss of 0.4428261553969501\n",
            "Na interação 4400 we have a loss of 0.43622106595231785\n",
            "Na interação 4500 we have a loss of 0.4298316021272378\n",
            "Na interação 4600 we have a loss of 0.42365313104687974\n",
            "Na interação 4700 we have a loss of 0.4176839138443089\n",
            "Na interação 4800 we have a loss of 0.41189781106416695\n",
            "Na interação 4900 we have a loss of 0.4062800474284035\n",
            "Na interação 5000 we have a loss of 0.4008405289950742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72Be-2An_shA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}